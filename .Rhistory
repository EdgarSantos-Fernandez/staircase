if(!missing(y)){
data$correct <- data[,names(data) == y]
}
# checks
if(missing(spat_model)){
spat_model = 'exp'
print('No spatial structure defined. Using an exponential model.
Other options are: sph, gau, car. For no spatial autocorrelation use "none"')
}
if(missing(itemtype)){
itemtype = '3PLUS'
print('No item response model defined. Using a 3PLUS.
Other options are: 2PLUS, 1PLUS, 3PL, 2PL and 1PL')
}
# Cov
if(spat_model == 'none' &
(itemtype == '1PLUS'| itemtype == '2PLUS' | itemtype == '3PLUS') ) stop("Cannot use spat_model == none with itemtype = 1PLUS, 2PLUS or 3PLUS. Use itemtype = 1PL, 2PL or 3PL")
# if (spat_model == 'none' & itemtype %in% c('1PLUS','2PLUS','3PLUS')){
#   stop("Cannot use spat_model == none with itemtype = 1PLUS, 2PLUS
#        or 3PLUS. Use itemtype = 1PL, 2PL or 3PL")
# }
if((spat_model == 'exp' | spat_model ==  'car') &
(itemtype == '1PL'| itemtype == '2PL' | itemtype == '3PL')){
stop("Cannot use spat_model == exp|car with itemtype = 1PL, 2PL or 3PL.
Use itemtype = 1PLUS, 2PLUS or 3PLUS")
}
# if (spat_model %in% c('exp', 'car') & itemtype %in% c('1PL','2PL','3PL')){
#   stop("Cannot use spat_model == exp|car with itemtype =
#        1PL, 2PL or 3PL. Use itemtype = 1PLUS, 2PLUS or 3PLUS")
# }
if(missing(abil)){
stop("Need to define the column containg the user id used to compute the ability")
}
if(missing(diff)){
stop("Need to define the column containg the image/item id used to compute the difficulty")
}
if(missing(seed)){
seed <- sample(1:1E6,1,replace=T)
}
if(missing(coords)){
stop("Please, specify the columns in the data frame with
the latitude and longitude (c('lon', 'lat'))")
}
data$lon <- data[,names(data) == coords[1], drop=TRUE]
data$lat <- data[,names(data) == coords[2], drop=TRUE]
data$siteID <- data[,names(data) == diff, drop=TRUE]
if(missing(formula)){
covariates <- 'no_cov'
}
if(!missing(formula)){
covariates <- 'use_cov'
}
##############################################################################
# Ensure data is in correct format
if (length(unique(data$id)) != max(unique(data$id))){
warning("The site ID's contain a site ID greater than the unique number of
ID's in the data. The data will be restructured so that the site
ID's are in sequence")
# Find the ID's which are missing from data
missingIDs <- which(unique(data$id) %notin% (1:length(unique(data$id))))
# Squish the ID's down to make them into a sequence
for (ID in rev(missingIDs)){
data$id <- data$id - (data$id > ID)
}
}
## Check that there are no different site ID's with same lat/long
if (nrow(unique(data[c("lon","lat")])) != nrow(unique(data[c("lon","lat","siteID")]))){
stop("There is a mismatch between the number of sites and the unique
longitude/latitude combinations")
}
##############################################################################
# Extract parameters in correct form
#  model <- 'exp'
#  sph
#  gau
#  car
cor_ed <- case_when(spat_model == "exp" ~ 1,
spat_model == "sph" ~ 2,
spat_model == "gau" ~ 3,
TRUE ~ 5)
cor_ed <- sort(cor_ed)[1]
cor_car <- case_when(spat_model == "car" ~ 1,
TRUE ~ 5)
cor_car <- sort(cor_car)[1]
data_com <-  'data {
int<lower=1> N;                       // num elicitation points classified
int<lower=1> M;                       // number of images and unique locations.
int<lower=1> Nindiv;                  // number users
int<lower=1> K;
int<lower=1,upper=M> id[N];           // image id
int<lower=1,upper=Nindiv> annot[N];   // user id
int<lower=0,upper=1> y[N];            // outcome variable
matrix[N,K] X ; //
matrix[M, M] e ; // Euclidean dist mat
matrix[M, M] I ; // diag matrix
real<lower=1> alpha_max ;
int<lower=1> Nspecies;                       // num species
int<lower=1,upper=Nspecies> species_id[N];           // species id
matrix[M, M] W ; // Adj mat
//int<lower=0> N_edges;
//int<lower=1, upper=N> node1[N_edges]; // node1[i] adjacent to node2[i]
//int<lower=1, upper=N> node2[N_edges]; // and node1[i] < node2[i]
}'
param_com <- '
parameters {
vector[K] beta; // regression coef
vector[Nindiv] abil;            // ability in the spatial model
real<lower=0>sigma_abil;        // sd of the abilities spatial model
vector<lower=0>[M] alpha;       // slope in the spatial model
real<lower=0> sigma_alpha;       // sd of the slope
vector<lower=0,upper=1>[Nspecies] eta; // guessing in the spatial model
vector[Nspecies] diff_species; // difficulty based on species
real<lower=0>sigma_diff_species; //
vector[M] difficulty;           // difficulty in the spatial model
'
param_ed <- '
real<lower=0> sigma_ed;
real<lower=0> alpha_ed; // range of the Euclidean dist model
real<lower=0> sigma_nug;
'
param_car <- '
real<lower=0> sigma_nug;
real<lower = 0> tau;
real<lower = 0, upper = 1> rho; // amount of temporal dependence
//real<lower=0> tau_phi;          // precision of spatial effects
//vector[M] phi;                  // spatial effects
'
param_nospat <- '
// vector[M] difficulty;           // difficulty in the spatial model
real<lower=0>mu_diff;           // mean of the difficulty
real<lower=0>sigma_diff;        // sd of the difficulty
'
tparam_com <- '
transformed parameters {
vector[M] mu; // mean
'
tparam_ed <- '
real<lower=0> var_ed; //  Euclidean dist var
matrix[M, M] C_ed ;// Euclidean cov
real<lower=0> var_nug; // nugget
'
tparam_car <- '
real<lower=0> var_nug; // nugget
//  vector[M] difficulty;           // difficulty in the spatial model
//   real<lower=0> sigma_phi = inv(sqrt(tau_phi));  // convert precision to sigma
//   difficulty = phi * sigma_phi;
'
tparam_com2 <- '
for (i in 1:N){
mu[id[i]] = X[i,] * beta;
}'
tparam_com_nocovs <- ' // if no covariates
for (i in 1:N){
mu[id[i]] = 0;
}
'
tparam_ed2 <- '
var_nug = sigma_nug ^ 2; // variance nugget
//Euclidean distance models start
var_ed = sigma_ed ^ 2; // var Euclidean dist
C_ed = var_ed * exp(- 3 * e / alpha_ed); // exponential model
//Euclidean distance models end
'
tparam_car2 <- '
var_nug = sigma_nug ^ 2; // variance nugget
//sigma_phi = inv(sqrt(tau_phi));  // convert precision to sigma
//difficulty = phi * sigma_phi;
'
model_com <- '
model {
vector [N] pijk;                // prob of correct classification in the spatial model
abil ~ normal(0, sigma_abil);  //  ; normal(0, 1) // informative prior for ability in spatial model
sigma_abil ~ uniform(0,10);  // cauchy(0,5);      // flat prior on the abilities sd
// mean(abil) ~ normal(0,0.001); // soft sum-to-zero constraint on abil
alpha ~ normal(1,sigma_alpha); //lognormal(0,sigma_alpha);            //    // discrimination. lognormal distribution. Does not work well
sigma_alpha ~  uniform(0,10);   //cauchy(0,5);      // // sd discrimination. Half Cauchy since sigma_alpha is defined above as possitive. See Stan manual
eta ~ beta(1,5);         //(2,14) (5,20) (1,5) prior on guessing spatial
diff_species ~ normal(0, sigma_diff_species);
sigma_diff_species ~  cauchy(0,5); //uniform(0,10);       // flat prior on the diff_species sd
mean(diff_species) ~ normal(0,0.001); // soft sum-to-zero constraint on diff_species
'
model_1PL <- '
for (i in 1 : N) {
pijk[i] = inv_logit( (abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) ) ;
y[i] ~ bernoulli( pijk[i]);
}
'
model_2PL <- '
for (i in 1 : N) {
pijk[i] = inv_logit(alpha[id[i]] * (abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) ) ;
y[i] ~ bernoulli( pijk[i]);
}
'
model_3PL <- '
for (i in 1 : N) {
pijk[i] = eta[species_id[i]] + (1 - eta[species_id[i]]) *
(inv_logit(alpha[id[i]] * (abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) ) );
y[i] ~ bernoulli( pijk[i]);
}
'
model_ed <- '
sigma_ed ~ uniform(0,100); // sd Euclidean dist
alpha_ed ~ uniform(0, alpha_max); // Euclidean dist range
sigma_nug ~ uniform(0, 50); //
target += multi_normal_cholesky_lpdf(difficulty | mu,
cholesky_decompose(C_ed + var_nug * I + 1e-9) );
'
model_car <- '
sigma_nug ~ uniform(0, 50); //
tau ~ gamma(2, 2);
//target += multi_normal_cholesky_lpdf(difficulty | mu,
//cholesky_decompose(tau * (I - rho * W) + var_nug * I + 1e-9) );
difficulty ~ multi_normal_prec(mu, tau * (I - rho * W) + var_nug * I + 1e-9); // using the precision
//tau_phi ~ gamma(1, 1) ;         // NOTE:  no prior on phi_raw, it is used to construct phi
// the following computes the prior on phi on the unit scale with sd = 1
// target += -0.5 * dot_self(phi[node1] - phi[node2]);
'
model_nospat <- '
difficulty ~ normal(mu_diff, sigma_diff); //informative true prior for ability in Rasch model
mu_diff ~ normal(0,5);          // prior on the mean difficulty
sigma_diff ~ cauchy(0,5);       // prior on the sd difficulty
'
generated_quantities_com <- 'generated quantities { // to compute the log likelihood
vector [N] lin;
vector[N] log_lik;
for (i in 1 : N) {
'
generated_quantities_1PL <- '
lin[i] = inv_logit(abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) ;
log_lik[i] = bernoulli_log(y[i], lin[i]);
}
}'
generated_quantities_2PL <- '
lin[i] = inv_logit(alpha[id[i]] * (abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) );
log_lik[i] = bernoulli_log(y[i], lin[i]);
}
}'
generated_quantities_3PL <- '
lin[i] = eta[species_id[i]] + (1 - eta[species_id[i]]) *
(inv_logit(alpha[id[i]] * (abil[annot[i]] -
(diff_species[species_id[i]] + difficulty[id[i]] )) ) );
log_lik[i] = bernoulli_log(y[i], lin[i]);
}
}'
if(cor_ed %in% 1:3) print('Fitting a Euclidean distance model (cor_ed)')
if(cor_car %in% 1:3) print('Fitting a CAR model (cor_car)')
if(spat_model == 'none') print('Fitting a no spatial model (no_spat)')
# builds a string with the model
ir_model <- paste(
data_com,
param_com,
if(cor_ed %in% 1:3) param_ed,
if(cor_car %in% 1:3) param_car,
if(spat_model == 'none') param_nospat,
'}',
tparam_com,
if(cor_ed %in% 1:3)tparam_ed,
if(cor_car %in% 1:3)tparam_car,
tparam_com2,
if(cor_ed %in% 1:3) tparam_ed2,
if(cor_car %in% 1:3) tparam_car2,
'}',
model_com,
if(itemtype %in% '1PLUS' | itemtype %in% '1PL' ) model_1PL,
if(itemtype %in% '2PLUS' | itemtype %in% '2PL' ) model_2PL,
if(itemtype %in% '3PLUS' | itemtype %in% '3PL' ) model_3PL,
if(cor_ed %in% 1:3) model_ed,
if(cor_car %in% 1:3) model_car,
if(spat_model == 'none') model_nospat,
'}',
generated_quantities_com,
if(itemtype %in% '1PLUS' | itemtype %in% '1PL' ) generated_quantities_1PL,
if(itemtype %in% '2PLUS' | itemtype %in% '2PL' ) generated_quantities_2PL,
if(itemtype %in% '3PLUS' | itemtype %in% '3PL' ) generated_quantities_3PL
)
# see model in notepad++. replace \n by @. then replace back @ by \r\n and tick Extend()
pars <- c(
case_when(cor_ed %in% 1:3 ~ c('var_ed',
'alpha_ed',
'var_nug'),
cor_ed %notin% 1:3 ~ ""),
case_when(cor_car %in% 1:3 ~ c('tau', 'rho'),
cor_car %notin% 1:3 ~ ""),
'beta',
'abil',
'difficulty',
'diff_species',
'alpha',
'eta',
if(loglik == T) 'log_lik',
'sigma_abil',
'sigma_alpha'
)
pars <- pars[pars != '']
if(covariates == 'use_cov'){
# data part
options(na.action='na.pass') # to preserve the NAs
out_list <- mylm(formula = formula, data = data) # produces the design matrix
response <- out_list$y # response variable
X <- out_list$X # design matrix
}
if(covariates != 'use_cov'){
# data part
response <- NULL
X <- matrix(1, nrow(data), 1)
}
# Total amount of locations
M <- length(unique(id))
# Convert the true classifications into numeric factors
data$True_Species_num <- as.numeric(as.factor(data$True_Species))
# spatial elements
# Euclidean distance
e <- data[c("lon", "lat", "id")] %>% distinct() %>%
dist(., method = "euclidean", diag = FALSE, upper = FALSE) %>% as.matrix()
# Car prior
# adjacency matrix
# Simplify2aray converts tibbles to arrays and arrays to arrays
# Remove tibbles and other data types (excessive calls to simplify2array()
# does not cause any issues)
nb2 <- voronoi(matrix(c(data$lon, data$lat), ncol=2))
#https://gis.stackexchange.com/questions/237810/adjacency-matrix-not-including-vertices
polyids = seq_along(nb2)
adjMat = matrix(FALSE, ncol = length(nb2), nrow = length(nb2))
for (ii in polyids) {
for (jj in setdiff(polyids, seq_len(ii))) {
adjMat[ii, jj] =
ifelse(class(gIntersection(nb2[ii, ], nb2[jj, ])) == 'SpatialLines',1,0)
}
}
#use symmetry for the other half
adjMat[lower.tri(adjMat)] = t(adjMat)[lower.tri(adjMat)]
W <- adjMat
num <- apply(W,1,sum)
adj <- c(unlist(apply(W,1,function(x) which(x == 1) ) ))
munged_data <- mungeCARdata4stan(adjBUGS = adj,numBUGS = num)
node1 = munged_data$node1
node2 = munged_data$node2
N_edges = munged_data$N_edges
#if(!is.null(slope) ) slope <- data[,slope]
#if(!is.null(slope) ) guessing <- data[,guessing]
Nspecies <- length(unique(data$True_Species_num))
data_list <- list(N = nrow(data), # total numb of anotations
M = M, # numb sites
Nindiv = length(unique(data$annotNum)), # numb users
K = ncol(X), # number of covariates + intercept
id = data$id, # image id
annot = data$annotNum, # users id
X = X, # design matrix of covariates
y = data[,"correct" ,drop=TRUE],
Nspecies = Nspecies,
species_id = data$True_Species_num,
#car part
W = W,
#N_edges= N_edges,
#node1 = node1,
#node2 = node2,
# Euc distance part
e = e,
I = diag(1, length(unique(data$id)), length(unique(data$id))),
alpha_max = 4 * max(e)
)
#RE1 = RE1mm # random effect matrix
ini <- function(){list(alpha = rep(1, M),
eta = rep(0.1, Nspecies)
)}
fit <- rstan::stan(model_code = ir_model,
model_name = "ir_model",
data = data_list,
pars = pars,
iter = iter,
warmup = warmup,
init = ini,
chains = chains,
verbose = F,
#seed = seed,
refresh = refresh
)
fit
}
fit <- ir_spat(formula = siteID ~ -1 + hard_bin, # covariates affecting the difficulty
data = data, # a data frame
spat_model = 'exp', # spatial covariance matrix
itemtype = '1PLUS', # item response model
abil = 'userID', # participants ids
diff = 'siteID', # location id
y = 'correct', # binary response variable
coords = c("lng", "lat"), # coordinates
iter = 2000,
warmup = 400,
chains = 3,
refresh = 10,
seed = seed
)
x = 1:nrow(data)
for (i in x){
a <- data$siteID[i]
if (a > 127){
data$siteID[i] <- a - 1
}
}
data <- data[,(1:23), drop=TRUE]
fit <- ir_spat(formula = siteID ~ -1 + hard_bin, # covariates affecting the difficulty
data = data, # a data frame
spat_model = 'exp', # spatial covariance matrix
itemtype = '1PLUS', # item response model
abil = 'userID', # participants ids
diff = 'siteID', # location id
y = 'correct', # binary response variable
coords = c("lng", "lat"), # coordinates
iter = 2000,
warmup = 400,
chains = 3,
refresh = 10,
seed = seed
)
data$correct <- 1000
x <- 1:nrow(data)
for (i in x){
if (data$cat[i] == data$cat_true[i]){
data$correct[i] <- 1
}else{
data$correct[i] <- 0
}
}
fit <- ir_spat(formula = siteID ~ -1 + hard_bin, # covariates affecting the difficulty
data = data, # a data frame
spat_model = 'exp', # spatial covariance matrix
itemtype = '1PLUS', # item response model
abil = 'userID', # participants ids
diff = 'siteID', # location id
y = 'correct', # binary response variable
coords = c("lng", "lat"), # coordinates
iter = 2000,
warmup = 400,
chains = 3,
refresh = 10,
seed = seed
)
names(data)
names(data)[18] <- "True_Species"
fit <- ir_spat(formula = siteID ~ -1 + hard_bin, # covariates affecting the difficulty
data = data, # a data frame
spat_model = 'exp', # spatial covariance matrix
itemtype = '1PLUS', # item response model
abil = 'userID', # participants ids
diff = 'siteID', # location id
y = 'correct', # binary response variable
coords = c("lng", "lat"), # coordinates
iter = 2000,
warmup = 400,
chains = 3,
refresh = 10,
seed = seed
)
abils <- stats[grep('abil\\[', rownames(stats)),]
abils <- cbind(abils, data %>% group_by(userID) %>% summarize(ns = n(), prop = mean(correct)) %>% data.frame())
names(abils)[grep('%', names(abils))] <- c('q2.5','q25', 'q50', 'q75', 'q97.5')
ggplot(abils, aes(x= prop, y = mean)) + geom_point() +
geom_text_repel(aes(x= prop, y = mean, label = userID), size = 2.5) +
xlab('Proportion of correct classification')+
ylab('Abilities')
diff <- stats[grep('difficulty\\[', rownames(stats)),]
diff <- cbind(diff, data %>% group_by(siteID, lng, lat) %>% summarize(ns = n(), prop = mean(correct)) %>% data.frame())
diff$diff_cat <- cut(diff$mean, breaks = c(-10,quantile(diff$mean)[2:4],10), 1:4, include.lowest = T)
cols = brewer.pal(5,'Reds')
ggplot(diff , aes(lng, lat, fill = diff_cat)) +
stat_voronoi(color="black") + scale_fill_manual(values=cols) +
geom_point() +
geom_text(aes(lng, lat, label = siteID), size = 2)+
coord_fixed(ratio=1)+
labs(fill = "Site difficulty")+
theme_bw()
species <- stats[grep('species\\[', rownames(stats)),]
species <- cbind(species, data %>% group_by(True_Species) %>% summarize(ns = n(), prop = mean(correct)) %>% data.frame())
names(species)[grep('%', names(species))] <- c('q2.5','q25', 'q50', 'q75', 'q97.5')
ggplot(species, aes(x= prop, y = mean)) +
geom_point() +
geom_errorbar(aes(ymin=q2.5, ymax=q97.5))+
xlab('Proportion of correct classification')+
ylab('Species difficulty')
names(abils)[grep('%', names(abils))] <- c('q2.5','q25', 'q50', 'q75', 'q97.5')
ggplot(abils, aes(x= prop, y = mean)) + geom_point() +
geom_text_repel(aes(x= prop, y = mean, label = userID), size = 2.5) +
xlab('Proportion of correct classification')+
ylab('Abilities')
mcmc_dens_overlay(
array,
pars = c(
"beta[1]"),
facet_args = list(nrow = 1)
)
mcmc_dens_overlay(
array,
pars = c(grep("beta", names(array), value = TRUE)),
facet_args = list(nrow = 1)
)
roxygen2::roxygenise()
library(staircase)
?mungeCARdata4stan
setwd("C:/Users/mcclymo2/OneDrive - Queensland University of Technology/Desktop")
f <- readRDS('serengety_gs.rds')
f <- readRDS('serengety_gs.rds')
?ir_spat
data <- readRDS('serengety_gs.rds')
data$correct
nrow(data$correct)
length(data$correct)
